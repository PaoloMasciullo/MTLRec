{
  "model": "PLE",
  "config": {
    "embedding_dim": 64,
    "num_tasks": 2,
    "num_layers": 2,
    "num_shared_experts": 1,
    "num_specific_experts": 2,
    "expert_hidden_units": [128, 64],
    "gate_hidden_units": [64, 32],
    "tower_hidden_units": [64, 32],
    "tower_output_sizes": [1, 1],
    "tower_output_activations": ["Sigmoid", "Sigmoid"],
    "hidden_activations": "ReLU",
    "dropout_probs": 0.5,
    "use_batchnorm": true
  }
}
